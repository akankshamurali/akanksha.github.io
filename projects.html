<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects – Akanksha Murali</title>

  <!-- Link to shared CSS -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>

  <!-- ========== HEADER ========== -->
  <header>
    <div class="container">
      <!-- Left: tabs -->
      <nav>
        <a href="index.html">Home</a>
        <a href="projects.html" class="active">Projects</a>
        <a href="resumes.html">Experience</a>
      </nav>

      <!-- Center: your name -->
      <h1>Akanksha Murali</h1>

      <!-- Right: social text links -->
      <div class="socials">
        <a href="https://www.linkedin.com/in/akanksha-murali/" target="_blank">LinkedIn</a>
        <a href="https://github.com/akankshamurali" target="_blank">GitHub</a>
        <a href="mailto:akankshamurali02@gmail.com">Email</a> 
      </div>
    </div>
  </header>

  <!-- ========== PROJECTS SECTION ========== -->
  <section class="section-wrapper projects-section" id="projects">
    <h2>Highlighted Projects</h2>
    <div class="project-cards">

      <!-- ==== PROJECT CARD #1 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/hexapod" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/hexy.jpeg"/>
            </div>
            <div class="card-content">
              <h3>Hexapod Autonomous Navigation</h3>
              <p>SLAM + MPC on a six-legged robot using ZED stereo camera and ROS2. Real-time pose logging, obstacle avoidance, and dynamic gait control.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #2 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/Traffic-Light-Control" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/traffic.jpg" />
            </div>
            <div class="card-content">
              <h3>Traffic Light Detection</h3>
              <p>Real-time computer vision pipeline (Python/OpenCV) for detecting and classifying traffic lights in video streams.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #3 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/Smart-Sorter" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/smartsorter.png" />
            </div>
            <div class="card-content">
              <h3>Smart Sorter System</h3>
              <p> A real-time object classification system that utilizes computer vision to sort items based on their shape and color. The system integrates a Raspberry Pi for image processing and an Arduino for hardware control, enabling automated sorting through motorized mechanisms.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #4 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/Visual-Navigation-and-Embodied-AI" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/target1.png" />
            </div>
            <div class="card-content">
              <h3>Visual Navigation and Embodied AI</h3>
              <p> This project implements an embodied AI agent capable of navigating a maze using visual input alone. The agent uses a camera feed to perceive its environment, extract semantic features, compute the optimal path, and autonomously navigate toward a goal.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #5 ==== -->
      <div class="project-card">
        <a class="full-link" https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-1" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/arucoimg2.png" />
            </div>
            <div class="card-content">
              <h3>Tag-based Augmented Reality: AR Cube Projection on ArUco Markers</h3>
              <p> This project demonstrates the implementation of tag-based augmented reality using ArUco markers. The program detects tags, estimates their pose, and overlays 3D objects such as a cube on the detected tags. It leverages OpenCV for marker detection, pose estimation, and visualization, showcasing augmented reality results from multiple perspectives. The implementation involves intrinsic camera calibration, marker size definitions, and detailed pose transformations for accurate AR projections.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #6 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-2" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/Task1_output_front.png" />
            </div>
            <div class="card-content">
              <h3>RANSAC-Based 3D Plane Fitting for Point Cloud Data</h3>
              <p> This project implements RANSAC (Random Sample Consensus) for fitting a plane to 3D point cloud data. The algorithm robustly estimates plane parameters while handling outliers, making it a valuable tool in computer vision and robotics. The implementation follows an iterative approach with inlier classification and adaptive sampling to optimize computation. Using Open3D, the results visualize fitted planes where inliers are marked in red and outliers in green, effectively identifying dominant planar structures in 3D space.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #7 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-1" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/data.png" />
            </div>
            <div class="card-content">
              <h3>Latent Embeddings & Classification</h3>
              <p> An autoencoder compressed Fashion-MNIST images into a low-dimensional latent space, which was visualized with t-SNE to show clear class clusters. A simple classifier trained on those latent features achieved 85.48 % test accuracy, with consistent loss/accuracy curves and a few example misclassifications.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #8 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-2" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/Task2_K1.png" />
            </div>
            <div class="card-content">
              <h3>ICP-Based 3D Point Cloud Alignment Using Open3D</h3>
              <p> This project implements the Iterative Closest Point (ICP) algorithm to align 3D point clouds, a crucial step in point cloud registration. It aligns two datasets by iteratively refining transformation estimates based on correspondences. The project includes two parts: one focusing on Open3D's demo point clouds and another applying ICP to KITTI self-driving dataset. The methodology involves correspondence matching, centroid alignment, rotation estimation using Singular Value Decomposition (SVD), and error convergence analysis. The results compare the accuracy of alignment between different datasets, highlighting ICP’s strengths and limitations.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #9 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-2" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/epipolarlines.png" />
            </div>
            <div class="card-content">
              <h3>F-Matrix and Relative Pose Estimation for Stereo Vision</h3>
              <p> This project computes the Fundamental Matrix (F-Matrix) and estimates the relative pose (R, t) between two stereo images. Using Aruco markers for robust point correspondences, the method applies RANSAC to filter outliers and accurately estimate the F-Matrix. The computed epipolar constraints are visualized to validate stereo geometry. The project further derives the Essential Matrix and decomposes it to recover the rotation and translation between the camera views. Visual results include epipolar line overlays on images, providing insights into the quality of estimated parameters.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #10 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-2" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/opticFlow_fr50.jpg" />
            </div>
            <div class="card-content">
              <h3>Object Tracking: Sparse and Dense Optical Flow</h3>
              <p> This project involves implementing Object Tracking across video sequences using Optical Flow algorithms. Two methods, the Lucas-Kanade Sparse Optical Flow and Farneback Dense Optical Flow, are explored. The project demonstrates tracking entities persistently, visualizing motion fields, and comparing the efficiency and accuracy of both methods. Detailed step-by-step explanations and results with bounding box tracking are provided.</p>
            </div>
          </div>
        </a>
      </div>

      <!-- ==== PROJECT CARD #11 ==== -->
      <div class="project-card">
        <a class="full-link" href="https://github.com/akankshamurali/NYUMSMR-23-25/tree/main/Robot%20Perception/Assignment-2" target="_blank">
          <div class="card-inner">
            <div class="card-image">
              <img src="assests/images/skiptrace.jpg" />
            </div>
            <div class="card-content">
              <h3>Skiptrace: Visual Place Recognition for Surveillance</h3>
              <p>This project implements a Visual Place Recognition system to match query images with a surveillance photo database using feature-based methods. The approach leverages SIFT descriptors, VLAD encoding, and k-means clustering to efficiently search and identify target locations from a large dataset. The implementation provides a structured pipeline for feature extraction, descriptor management, query processing, and visualization of retrieved images.</p>
            </div>
          </div>
        </a>
      </div>
      


  

    </div>
  </section>

  <!-- ========== FOOTER ========== -->
  <footer>
    <p>© 2025 Akanksha Murali. All rights reserved.</p>
  </footer>

</body>
</html>
